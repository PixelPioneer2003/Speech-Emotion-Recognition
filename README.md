# üéôÔ∏è Speech Emotion Recognition using TESS Dataset

## üìå Project Overview

This project focuses on building a **Speech Emotion Recognition (SER)** system using Deep Learning. The goal is to classify emotional states from speech signals, a critical component for emotionally intelligent systems like AI assistants, healthcare bots, and human-computer interaction tools.

The project uses the **Toronto Emotional Speech Set (TESS)**, which provides high-quality emotional speech samples recorded by two actresses portraying seven distinct emotions.

---

## üéß Dataset: Toronto Emotional Speech Set (TESS)

- **Download Link**: [TESS Dataset on Kaggle](https://www.kaggle.com/ejlok1/toronto-emotional-speech-set-tess)  
- **Additional Dataset**: [Speech Emotion Recognition Dataset](https://www.kaggle.com/dmitrybabko/speech-emotion-recognition-en)

### Dataset Description

- A set of **200 target words** were spoken using the phrase:  
  `"Say the word _"`
- Recordings were made by **two actresses**, aged **26** and **64**.
- Each word was spoken in **seven emotional states**:
  - `anger`
  - `disgust`
  - `fear`
  - `happiness`
  - `pleasant surprise`
  - `sadness`
  - `neutral`
- A total of **2800 audio recordings** in `.wav` format.

### Dataset Structure


---

## üéØ Project Objectives

‚úÖ Preprocess and extract relevant audio features (e.g., MFCCs, Chroma)  
‚úÖ Train Deep Learning models like LSTM and Neural Networks for emotion classification  
‚úÖ Visualize emotion distributions and audio spectrograms  
‚úÖ Evaluate model performance using accuracy, F1-score, and confusion matrix  

---

## ‚öôÔ∏è Tech Stack and Libraries

- **Programming Language**: Python
- **Libraries Used**:
  - `pandas` - Data processing
  - `matplotlib` - Data visualization
  - `librosa` - Audio feature extraction
  - `tensorflow` & `keras` - Deep Learning models

- **Models Used**:
  - Neural Networks (Fully Connected)
  - LSTM (Long Short-Term Memory Networks)

---



