# 🎙️ Speech Emotion Recognition using TESS Dataset

## 📌 Project Overview

This project focuses on building a **Speech Emotion Recognition (SER)** system using Deep Learning. The goal is to classify emotional states from speech signals, a critical component for emotionally intelligent systems like AI assistants, healthcare bots, and human-computer interaction tools.

The project uses the **Toronto Emotional Speech Set (TESS)**, which provides high-quality emotional speech samples recorded by two actresses portraying seven distinct emotions.

---

## 🎧 Dataset: Toronto Emotional Speech Set (TESS)

- **Download Link**: [TESS Dataset on Kaggle](https://www.kaggle.com/ejlok1/toronto-emotional-speech-set-tess)  
- **Additional Dataset**: [Speech Emotion Recognition Dataset](https://www.kaggle.com/dmitrybabko/speech-emotion-recognition-en)

### Dataset Description

- A set of **200 target words** were spoken using the phrase:  
  `"Say the word _"`
- Recordings were made by **two actresses**, aged **26** and **64**.
- Each word was spoken in **seven emotional states**:
  - `anger`
  - `disgust`
  - `fear`
  - `happiness`
  - `pleasant surprise`
  - `sadness`
  - `neutral`
- A total of **2800 audio recordings** in `.wav` format.

### Dataset Structure


---

## 🎯 Project Objectives

✅ Preprocess and extract relevant audio features (e.g., MFCCs, Chroma)  
✅ Train Deep Learning models like LSTM and Neural Networks for emotion classification  
✅ Visualize emotion distributions and audio spectrograms  
✅ Evaluate model performance using accuracy, F1-score, and confusion matrix  

---

## ⚙️ Tech Stack and Libraries

- **Programming Language**: Python
- **Libraries Used**:
  - `pandas` - Data processing
  - `matplotlib` - Data visualization
  - `librosa` - Audio feature extraction
  - `tensorflow` & `keras` - Deep Learning models

- **Models Used**:
  - Neural Networks (Fully Connected)
  - LSTM (Long Short-Term Memory Networks)

---

## 🏗️ Project Structure

